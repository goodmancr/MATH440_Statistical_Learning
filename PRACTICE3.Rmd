---
title: "Practice Quiz 3"
author: "Caroline Goodman"
date: "2022-09-25"
output:
  html_document: default
  pdf_document: default
---
## Consider the diabetes data set. There are 769 observations and 9 variables. The objective is to forecast outcome using the other regressors.

## Create a 70:20:10 partition for training, validation and test data.

```{r}
Dat <- read.csv("~/Desktop/MATH 440 - Statistical Learning/diabetes.csv")
source("~/Desktop/MATH 440 - Statistical Learning/myfunctions.R")

# create training and testing data

RNGkind (sample.kind = "Rounding") 
set.seed(0) ## set seed so that you get same partition each time
p3 <- partition.3(Dat, 0.7, 0.2) ## creating 70:20:10 partition
training.data <- p3$data.train
validation.data <- p3$data.val
test.data <- p3$data.test
```


## Fit a logistic regression model on the training data for predicting outcome using the other regressors.
## Using alpha=0.05, identify the significant predictors.

<span style="color: red;">Answer from Output: </span>
Logistic Regression Model: 
    $y = 0.1306*Pregnancies + 0.0378*Glucose - 0.0126*BloodPressure$
       $- 0.0029*SkinThickness - 0.0018*Insulin + 0.0890*BMI$
        $+ 0.7763*DiabetesPedigreeFunction + 0.0138*Age$

Signficant Predictors: Pregnancies, Glucose, Blood Pressure, BMI, DiabetesPedigreeFunction

```{r}
logistic.model <- glm(Outcome ~ ., family = binomial(link = 'logit'), data = training.data)
summary(logistic.model)
confint(logistic.model)
exp(confint.default(logistic.model))
```


## How does the odds of being diagnosed with diabetes change if there is an one unit increase in BMI?

<span style="color: red;">Answer from Output: </span> 1.093072

```{r}
exp(0.088992)
```

## How does the odds of being diagnosed with diabetes change if age increases by 10 years?

<span style="color: red;">Answer from Output: </span> 1.147654

```{r}
exp(10 * 0.013772)
```

## For a 50 year old person with no pregnancies and Glucose = 90, BloodPressure = 74, SkinThickness = 23, Insulin = 0,  BMI = 33,  DiabetesPedigreeFunction = 0.7, predict the outcome. Use cutoff = 0.5.

<span style="color: red;">Answer from Output: </span> 0.1301474; because the estimated success probability is < 0.05, the estimated response is 0

```{r}
library(caret)
x0 <- data.frame(Glucose = 90, BloodPressure = 74, SkinThickness = 23, Insulin = 0,  BMI = 33,  DiabetesPedigreeFunction = 0.7, Pregnancies = 0, Age = 50)
predict(logistic.model, newdata = x0, type = "response")

```

## Create a confusion matrix on the test data. Report overall accuracy, sensitivity, specificity, and Kappa statistic. 

<span style="color: red;">Answer from Output: </span>
     Accuracy : 0.8158
     Sensitivity : 0.7083          
     Specificity : 0.8654 
     Kappa : 0.5737  

```{r}
pred.prob.test <- predict(logistic.model, newdata = test.data,type = "response")
pred.y.test <- ifelse(pred.prob.test > 0.5, 1, 0) # using cutoff = 0.5
confusionMatrix(as.factor(pred.y.test), as.factor(test.data$Outcome), 
                positive = "1")
```

## Try different cutoff values (0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9) and create a confusion matrix for validation data. Select the best cutoff for this problem based on the Kappa coefficient. Report the best cutoff.

<span style="color: red;">Answer from Output: </span> 0.4 best cutoff value because the kappa value is 0.5422
```{r}
pred.prob.validation <- predict(logistic.model, newdata = validation.data, type = "response")
pred.y.validation <- ifelse(pred.prob.validation > 0.1, 1, 0) # using cutoff value 0.1
confusionMatrix(as.factor(pred.y.validation), as.factor(validation.data$Outcome), positive = "1")

pred.prob.validation <- predict(logistic.model, newdata = validation.data, type = "response")
pred.y.validation <- ifelse(pred.prob.validation > 0.2, 1, 0) # using cutoff value 0.2
confusionMatrix(as.factor(pred.y.validation), as.factor(validation.data$Outcome), positive = "1")

pred.prob.validation <- predict(logistic.model, newdata = validation.data, type = "response")
pred.y.validation <- ifelse(pred.prob.validation > 0.3, 1, 0) # using cutoff value 0.3
confusionMatrix(as.factor(pred.y.validation), as.factor(validation.data$Outcome), positive = "1")

pred.prob.validation <- predict(logistic.model, newdata = validation.data, type = "response")
pred.y.validation <- ifelse(pred.prob.validation > 0.4, 1, 0) # using cutoff value 0.4
confusionMatrix(as.factor(pred.y.validation), as.factor(validation.data$Outcome), positive = "1")

pred.prob.validation <- predict(logistic.model, newdata = validation.data, type = "response")
pred.y.validation <- ifelse(pred.prob.validation > 0.5, 1, 0) # using cutoff value 0.5
confusionMatrix(as.factor(pred.y.validation), as.factor(validation.data$Outcome), positive = "1")

pred.prob.validation <- predict(logistic.model, newdata = validation.data, type = "response")
pred.y.validation <- ifelse(pred.prob.validation > 0.6, 1, 0) # using cutoff value 0.6
confusionMatrix(as.factor(pred.y.validation), as.factor(validation.data$Outcome), positive = "1")

pred.prob.validation <- predict(logistic.model, newdata = validation.data, type = "response")
pred.y.validation <- ifelse(pred.prob.validation > 0.7, 1, 0) # using cutoff value 0.7
confusionMatrix(as.factor(pred.y.validation), as.factor(validation.data$Outcome), positive = "1")

pred.prob.validation <- predict(logistic.model, newdata = validation.data, type = "response")
pred.y.validation <- ifelse(pred.prob.validation > 0.8, 1, 0) # using cutoff value 0.8
confusionMatrix(as.factor(pred.y.validation), as.factor(validation.data$Outcome), positive = "1")

pred.prob.validation <- predict(logistic.model, newdata = validation.data, type = "response")
pred.y.validation <- ifelse(pred.prob.validation > 0.9, 1, 0) # using cutoff value 0.9
confusionMatrix(as.factor(pred.y.validation), as.factor(validation.data$Outcome), positive = "1")
```

## Combine training and validation data. Fit logistic regression model on the combined data using the best cutoff. Test this model on the test data and report overall accuracy, sensitivity, specificity, and Kappa statistic. 

<span style="color: red;">Answer from Output: </span>

  Accuracy : 0.7587           
  Sensitivity : 0.6762         
  Specificity : 0.8036                    
  Kappa : 0.4758      

```{r}
combined <- rbind(training.data, validation.data)
## fit new model
combined.model <- glm(Outcome ~ ., data = combined, family = binomial(link='logit'))
## test model on test data
pred.prob.test <- predict(combined.model, newdata = test.data, type = "response")
pred.y.test <- ifelse(pred.prob.test > 0.4, 1, 0) # using cutoff value 0.4
confusionMatrix(as.factor(pred.y.test), as.factor(test.data$Outcome), positive = "1")
```

## Create a ROC chart for the model fitted in the above question on the test data. Report the area under the curve.

<span style="color: red;">Answer from Output: </span>
area under the curve on the test data: 0.8606           
area under the curve on the combined data: 0.8355

```{r}
library(pROC)

# on test data
r <- roc(test.data$Outcome, pred.prob.test)
plot.roc(r, main = "Test ROC curve")
auc(r)
```

## Create a lift chart for the model fitted in the above question on the test data. What percentage of people with positive diabetes outcome is captured in the top 30%?

<span style="color: red;">Answer from Output: </span> 66.7% of people with positive diabetes outcome is captured in the top 30%

```{r}
library(gains)
gain <- gains(test.data$Outcome, pred.prob.test)
gain

x <- c(0, gain$depth)
pred.y <- c(0, gain$cume.pct.of.total)
avg.y <- c(0, gain$depth/100)
plot(x, pred.y, main = "Cumulative Lift Chart", xlab = "deciles", ylab = "Percent cumulative response", type = "l", col = "red", cex.lab = 1.5)
lines(x, avg.y, type = "l")
```

